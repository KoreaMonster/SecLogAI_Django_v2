#!/usr/bin/env python3
"""
Django ì—°ë™ ML ê¸°ë°˜ ë¡œê·¸ ì •ë³´ ì¶”ì¶œ ì‹œìŠ¤í…œ
ê¸°ì¡´ ml_log_classifier.pyë¥¼ Django ORMê³¼ ì—°ë™í•˜ë„ë¡ ìµœì†Œ ìˆ˜ì •
"""

import json
import logging
from datetime import datetime
from typing import Dict, Optional, Tuple, List
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
import joblib
import numpy as np
import re
from collections import Counter

# Django imports
from .models import LogFile, LogEntry
from django.db import models

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ImprovedMLLogExtractor:
    """ê¸°ì¡´ ML ê¸°ë°˜ ë¡œê·¸ ì •ë³´ ì¶”ì¶œê¸° (Django ì—°ë™)"""

    def __init__(self):
        self.log_classifier = None
        self.tfidf_vectorizer = None
        self.trained = False

    def extract_enhanced_features(self, text):
        """í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)"""
        features = []

        # ê¸°ë³¸ í…ìŠ¤íŠ¸ íŠ¹ì„±
        features.append(len(text))
        features.append(len(text.split()))
        features.append(text.count(' '))
        features.append(text.count(':'))
        features.append(text.count('['))
        features.append(text.count(']'))
        features.append(text.count('"'))
        features.append(text.count('-'))
        features.append(text.count('='))
        features.append(text.count('/'))

        # êµ¬ì¡°ì  íŠ¹ì„± (ê°€ì¤‘ì¹˜ ê°•í™”)
        digit_count = sum(c.isdigit() for c in text)
        alpha_count = sum(c.isalpha() for c in text)
        features.append(digit_count / len(text) if text else 0)
        features.append(alpha_count / len(text) if text else 0)

        # ë³´ì•ˆ ê´€ë ¨ í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ì¦ê°€)
        security_keywords = ['ERROR', 'WARN', 'INFO', 'failed', 'denied', 'login',
                             'access', 'file', 'user', 'attempt', 'blocked']
        for keyword in security_keywords:
            features.append(2 if keyword.lower() in text.lower() else 0)

        # ì›¹ ë¡œê·¸ íŠ¹ì„±
        web_keywords = ['GET', 'POST', 'HTTP', 'html', 'php', 'css', 'js']
        for keyword in web_keywords:
            features.append(1 if keyword in text.upper() else 0)

        # ì‹œìŠ¤í…œ ë¡œê·¸ íŠ¹ì„±
        system_keywords = ['kernel', 'systemd', 'sshd', 'mysqld']
        for keyword in system_keywords:
            features.append(1 if keyword.lower() in text.lower() else 0)

        # ë°©í™”ë²½ íŠ¹ì„±
        firewall_keywords = ['TRAFFIC', 'THREAT', 'ALLOW', 'DENY', 'firewall', 'IPTABLES']
        for keyword in firewall_keywords:
            features.append(1 if keyword.upper() in text.upper() else 0)

        # êµ¬ì¡° íŒ¨í„´
        features.append(1 if text and text[0].isdigit() else 0)
        features.append(1 if text.startswith('{') else 0)
        features.append(1 if text.startswith('[') else 0)
        features.append(1 if 'HTTP' in text and any(m in text for m in ['GET', 'POST']) else 0)

        return features

    def load_and_analyze_training_data(self, file_path):
        """ê¸°ì¡´ í•™ìŠµ ë°ì´í„° ë¶„ì„ ë¡œì§ ìœ ì§€"""
        print("ğŸ“š í•™ìŠµ ë°ì´í„° ë¡œë“œ ì¤‘...")

        logs = []
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#'):
                    logs.append(line)

        print(f"ğŸ“Š ì´ {len(logs)}ê°œ ë¡œê·¸ ë¡œë“œë¨")

        # í–¥ìƒëœ íŠ¹ì„± ì¶”ì¶œ
        features = [self.extract_enhanced_features(log) for log in logs]

        # TF-IDF ë²¡í„°í™”
        self.tfidf_vectorizer = TfidfVectorizer(
            max_features=300,
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.8
        )
        tfidf_features = self.tfidf_vectorizer.fit_transform(logs).toarray()

        # íŠ¹ì„± ê²°í•©
        combined_features = np.hstack([features, tfidf_features])

        # K-means í´ëŸ¬ìŠ¤í„°ë§
        n_clusters = 4
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(combined_features)

        # í´ëŸ¬ìŠ¤í„° ë¼ë²¨ë§
        cluster_to_type = {}
        for i in range(n_clusters):
            cluster_logs = [logs[j] for j in range(len(logs)) if cluster_labels[j] == i]
            log_type = self.improved_log_type_detection(cluster_logs)
            cluster_to_type[i] = log_type

            print(f"í´ëŸ¬ìŠ¤í„° {i} -> {log_type}: {len(cluster_logs)}ê°œ")
            for sample in cluster_logs[:2]:
                print(f"  ì˜ˆì‹œ: {sample[:80]}...")

        # ìµœì¢… ë¼ë²¨ë§
        log_types = [cluster_to_type[label] for label in cluster_labels]

        # ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ
        self.log_classifier = RandomForestClassifier(n_estimators=150, random_state=42, max_depth=10)
        self.log_classifier.fit(combined_features, log_types)

        print("ğŸ¤– ML ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        self.trained = True

        return logs, log_types

    def improved_log_type_detection(self, samples):
        """ê¸°ì¡´ ë¡œê·¸ íƒ€ì… ê²°ì • ë¡œì§ ìœ ì§€"""
        combined_text = ' '.join(samples).upper()

        scores = {
            'security_event': 0,
            'apache': 0,
            'nginx': 0,
            'syslog': 0,
            'firewall': 0,
            'application': 0
        }

        # ë³´ì•ˆ ì´ë²¤íŠ¸ ì ìˆ˜
        security_terms = ['ERROR', 'WARN', 'FAILED', 'DENIED', 'LOGIN', 'FILE', 'USER', 'MALWARE', 'SQL', 'ATTEMPT']
        for term in security_terms:
            if term in combined_text:
                scores['security_event'] += 2

        # ì›¹ ë¡œê·¸ ì ìˆ˜
        if 'HTTP' in combined_text:
            if any(method in combined_text for method in ['GET', 'POST', 'PUT', 'DELETE']):
                if '- -' in combined_text or 'HTTPD' in combined_text:
                    scores['apache'] += 3
                else:
                    scores['nginx'] += 3

        # ì‹œìŠ¤í…œ ë¡œê·¸ ì ìˆ˜
        system_terms = ['KERNEL', 'SYSTEMD', 'SSHD', 'CRON']
        for term in system_terms:
            if term in combined_text:
                scores['syslog'] += 3

        # ë°©í™”ë²½ ì ìˆ˜
        fw_terms = ['TRAFFIC', 'THREAT', 'FIREWALL', 'IPTABLES', 'ALLOW', 'DENY']
        for term in fw_terms:
            if term in combined_text:
                scores['firewall'] += 3

        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ ì ìˆ˜
        if any(term in combined_text for term in ['EXEC', 'CONTROLLER', 'SPRING']):
            scores['application'] += 2

        best_type = max(scores, key=scores.get)
        if scores[best_type] < 2:
            return 'unknown'

        return best_type

    def classify_log(self, log_text):
        """ê¸°ì¡´ ë¡œê·¸ ë¶„ë¥˜ ë¡œì§ ìœ ì§€"""
        if not self.trained:
            return 'unknown'

        try:
            enhanced_features = self.extract_enhanced_features(log_text)
            tfidf_features = self.tfidf_vectorizer.transform([log_text]).toarray()[0]
            combined_features = np.hstack([enhanced_features, tfidf_features]).reshape(1, -1)

            prediction = self.log_classifier.predict(combined_features)[0]
            probabilities = self.log_classifier.predict_proba(combined_features)[0]
            confidence = max(probabilities)

            if confidence < 0.6:
                return 'unknown'

            return prediction
        except:
            return 'unknown'

    def extract_ip_from_log(self, log_text):
        """ê¸°ì¡´ IP ì¶”ì¶œ ë¡œì§ ìœ ì§€"""
        words = log_text.replace(',', ' ').replace(':', ' ').replace(';', ' ').split()

        for word in words:
            clean_word = word.strip('[]()"\',')
            parts = clean_word.split('.')

            if len(parts) == 4:
                try:
                    nums = [int(part) for part in parts]
                    if all(0 <= num <= 255 for num in nums):
                        ip = '.'.join(parts)
                        if not ip.startswith(('0.0.0', '255.255.255')):
                            return ip
                except (ValueError, IndexError):
                    continue

        return None

    def extract_timestamp_from_log(self, log_text):
        """ê¸°ì¡´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ ë¡œì§ ìœ ì§€"""
        timestamp_patterns = [
            (r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', "%Y-%m-%d %H:%M:%S"),
            (r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2})', "%Y-%m-%dT%H:%M:%S"),
            (r'\[(\d{2}/\w{3}/\d{4}:\d{2}:\d{2}:\d{2})', "%d/%b/%Y:%H:%M:%S"),
            (r'^(\w{3} \d{1,2} \d{2}:\d{2}:\d{2})', "%b %d %H:%M:%S")
        ]

        for pattern, format_str in timestamp_patterns:
            match = re.search(pattern, log_text)
            if match:
                try:
                    time_str = match.group(1)
                    if format_str == "%b %d %H:%M:%S":
                        current_year = datetime.now().year
                        time_str = f"{current_year} {time_str}"
                        format_str = f"%Y {format_str}"
                    return datetime.strptime(time_str, format_str)
                except:
                    continue

        return datetime.now()

    def extract_severity_from_log(self, log_text, log_type):
        """ê¸°ì¡´ ì‹¬ê°ë„ ì¶”ì¶œ ë¡œì§ ìœ ì§€"""
        text_upper = log_text.upper()

        high_risk = ['ERROR', 'CRITICAL', 'FATAL', 'FAILED', 'DENIED', 'MALWARE', 'SQL', 'INJECTION', 'EXPLOIT',
                     'ATTACK']
        medium_risk = ['WARN', 'WARNING', 'TIMEOUT', 'ANOMALOUS', 'SUSPICIOUS', 'BLOCKED']
        low_risk = ['INFO', 'DEBUG', 'TRACE', 'COMPLETED', 'SUCCESS']

        score = 0
        for keyword in high_risk:
            if keyword in text_upper:
                score += 3
        for keyword in medium_risk:
            if keyword in text_upper:
                score += 2
        for keyword in low_risk:
            if keyword in text_upper:
                score += 1

        if score >= 5:
            return 'high'
        elif score >= 3:
            return 'medium'
        elif score >= 1:
            return 'low'
        else:
            return 'info'

    def build_message_from_log(self, log_text, log_type):
        """ê¸°ì¡´ ë©”ì‹œì§€ êµ¬ì„± ë¡œì§ ìœ ì§€"""
        if log_type in ['apache', 'nginx']:
            method = None
            path = None
            status = None

            parts = log_text.split()
            for i, part in enumerate(parts):
                if part in ['GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'OPTIONS']:
                    method = part
                    if i + 1 < len(parts):
                        path = parts[i + 1].strip('"').split()[0] if parts[i + 1] else '/'
                elif part.isdigit() and len(part) == 3 and 100 <= int(part) <= 599:
                    status = part

            return f"{method or 'GET'} {path or '/'} â†’ {status or '200'}"

        elif log_type == 'security_event':
            key_parts = []
            if 'ERROR' in log_text:
                key_parts.append('ERROR')
            if 'failed' in log_text.lower():
                key_parts.append('Login Failed')
            if 'file' in log_text.lower():
                key_parts.append('File Access')
            if 'sql' in log_text.lower():
                key_parts.append('SQL Injection')
            if 'malware' in log_text.lower():
                key_parts.append('Malware Detected')

            if key_parts:
                return ' | '.join(key_parts)
            return log_text[:100] + '...' if len(log_text) > 100 else log_text

        return log_text[:100] + '...' if len(log_text) > 100 else log_text

    def build_metadata_from_log(self, log_text, log_type):
        """ê¸°ì¡´ ë©”íƒ€ë°ì´í„° êµ¬ì„± ë¡œì§ ìœ ì§€"""
        metadata = {'log_format': log_type}

        if log_type in ['apache', 'nginx']:
            parts = log_text.split()
            for part in parts:
                if part in ['GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'OPTIONS']:
                    metadata['method'] = part
                elif part.isdigit() and len(part) == 3:
                    try:
                        metadata['status_code'] = int(part)
                    except:
                        pass

        return metadata

    def extract_all_info(self, log_line):
        """ê¸°ì¡´ ì •ë³´ ì¶”ì¶œ ë¡œì§ ìœ ì§€"""
        log_type = self.classify_log(log_line)
        timestamp = self.extract_timestamp_from_log(log_line)
        source_ip = self.extract_ip_from_log(log_line)
        severity = self.extract_severity_from_log(log_line, log_type)
        message = self.build_message_from_log(log_line, log_type)
        metadata = self.build_metadata_from_log(log_line, log_type)

        return {
            'timestamp': timestamp,
            'log_type': log_type,
            'source_ip': source_ip,
            'message': message,
            'severity': severity,
            'raw_log': log_line,
            'metadata': json.dumps(metadata)
        }


class LogMLService:
    """Django ì—°ë™ ML ë¡œê·¸ ì²˜ë¦¬ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.extractor = ImprovedMLLogExtractor()

    def process_uploaded_file(self, log_file_instance: LogFile) -> Tuple[int, int]:
        """
        ì—…ë¡œë“œëœ LogFileì„ MLë¡œ ì²˜ë¦¬í•˜ì—¬ LogEntryë“¤ ìƒì„±
        ê¸°ì¡´ process_file ë¡œì§ì„ Django ORMìœ¼ë¡œ ìˆ˜ì •
        """
        file_path = log_file_instance.file.path
        processed = 0
        failed = 0

        print(f"ğŸ“ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path}")

        # ML ëª¨ë¸ í•™ìŠµ (ê¸°ì¡´ ë¡œì§)
        try:
            self.extractor.load_and_analyze_training_data(file_path)
        except Exception as e:
            logger.error(f"ML í•™ìŠµ ì‹¤íŒ¨: {e}")
            return 0, 1

        # ë¡œê·¸ ì²˜ë¦¬ ë° Django ëª¨ë¸ ì €ì¥ (í•µì‹¬ ìˆ˜ì • ë¶€ë¶„)
        log_entries_to_create = []

        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if not line or line.startswith('#'):
                    continue

                try:
                    # ê¸°ì¡´ ML ì²˜ë¦¬ ë¡œì§
                    extracted_info = self.extractor.extract_all_info(line)

                    # Django LogEntry ê°ì²´ ìƒì„± (SQLite ëŒ€ì‹ )
                    log_entry = LogEntry(
                        log_file=log_file_instance,
                        timestamp=extracted_info['timestamp'],
                        log_type=extracted_info['log_type'],
                        source_ip=extracted_info['source_ip'],
                        message=extracted_info['message'],
                        severity=extracted_info['severity'],
                        raw_log=extracted_info['raw_log'],
                        metadata=extracted_info['metadata']
                    )
                    log_entries_to_create.append(log_entry)
                    processed += 1

                except Exception as e:
                    logger.error(f"ë¡œê·¸ ì²˜ë¦¬ ì˜¤ë¥˜ (ë¼ì¸ {line_num}): {e}")
                    failed += 1

                # ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                if len(log_entries_to_create) >= 1000:
                    LogEntry.objects.bulk_create(log_entries_to_create)
                    log_entries_to_create = []
                    print(f"â³ ì²˜ë¦¬ì¤‘: {line_num} ë¼ì¸...")

        # ë‚¨ì€ ë¡œê·¸ ì—”íŠ¸ë¦¬ë“¤ ì €ì¥
        if log_entries_to_create:
            LogEntry.objects.bulk_create(log_entries_to_create)

        # LogFile ì •ë³´ ì—…ë°ì´íŠ¸
        log_file_instance.total_entries = processed
        log_file_instance.save()

        print(f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {processed}ê°œ ì„±ê³µ, {failed}ê°œ ì‹¤íŒ¨")
        return processed, failed

    def get_processing_stats(self, log_file_instance: LogFile) -> Dict:
        """ì²˜ë¦¬ í†µê³„ ì¡°íšŒ"""
        entries = LogEntry.objects.filter(log_file=log_file_instance)

        # log_typeë³„ ì¹´ìš´íŠ¸ ì§‘ê³„
        log_type_counts = entries.values('log_type').annotate(count=models.Count('id'))
        log_type_distribution = {item['log_type']: item['count'] for item in log_type_counts}

        # severityë³„ ì¹´ìš´íŠ¸ ì§‘ê³„
        severity_counts = entries.values('severity').annotate(count=models.Count('id'))
        severity_distribution = {item['severity']: item['count'] for item in severity_counts}

        return {
            'total_entries': entries.count(),
            'log_type_distribution': log_type_distribution,
            'severity_distribution': severity_distribution,
            'unique_ips': entries.values('source_ip').distinct().count()
        }