#!/usr/bin/env python3
"""
Django ê¸°ë°˜ ìƒê´€ê´€ê³„ ë¶„ì„ê¸°
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from collections import defaultdict, Counter
from typing import Dict, List, Optional, Tuple
from django.apps import apps

plt.rcParams['font.family'] = 'DejaVu Sans'
sns.set_style("whitegrid")


class CorrelationAnalyzer:
    def __init__(self):
        # Django ORM ì‚¬ìš©
        LogEntry = apps.get_model('logs', 'LogEntry')
        self.LogEntry = LogEntry
        self.df = None
        self._load_data()

        self.thresholds = {
            'correlation_threshold': 0.7,
            'sequence_gap_minutes': 30,
            'clustering_min_events': 5
        }

    def _load_data(self):
        """Django ORMìœ¼ë¡œ ë°ì´í„° ë¡œë“œ"""
        entries = self.LogEntry.objects.all().values(
            'timestamp', 'log_type', 'source_ip', 'message', 'severity', 'metadata'
        )
        self.df = pd.DataFrame(entries)

        if len(self.df) > 0:
            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], errors='coerce')
            self.df = self.df.dropna(subset=['timestamp'])
            self.df['hour'] = self.df['timestamp'].dt.hour

    def get_correlation_overview_text(self) -> str:
        """ìƒê´€ê´€ê³„ ë¶„ì„ ê°œìš”"""
        if len(self.df) == 0:
            return "ğŸ”— ë¶„ì„í•  ìƒê´€ê´€ê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        total_logs = len(self.df)
        unique_ips = self.df['source_ip'].dropna().nunique()
        unique_log_types = self.df['log_type'].nunique()
        time_span = (self.df['timestamp'].max() - self.df['timestamp'].min()).days

        overview = f"""
ğŸ”— ìƒê´€ê´€ê³„ ë¶„ì„ (Django ORM)
{'=' * 30}
â€¢ ì´ ë¡œê·¸ ìˆ˜: {total_logs:,}ê°œ
â€¢ ê³ ìœ  IP ìˆ˜: {unique_ips}ê°œ
â€¢ ë¡œê·¸ íƒ€ì…: {unique_log_types}ê°œ
â€¢ ë¶„ì„ ê¸°ê°„: {time_span}ì¼
"""
        return overview

    def analyze_temporal_correlation(self) -> Tuple[Dict, str]:
        """ì‹œê°„ì  ìƒê´€ê´€ê³„ ë¶„ì„"""
        if len(self.df) == 0:
            return {}, "ì‹œê°„ì  ìƒê´€ê´€ê³„ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ì‹œê°„ë³„ ë¡œê·¸ íƒ€ì… ë¶„í¬
        time_log_type = self.df.groupby(['hour', 'log_type']).size().unstack(fill_value=0)

        # ìƒê´€ê´€ê³„ ê³„ì‚°
        correlation_matrix = time_log_type.corr()

        # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i + 1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) >= self.thresholds['correlation_threshold']:
                    strong_correlations.append({
                        'log_type_1': correlation_matrix.columns[i],
                        'log_type_2': correlation_matrix.columns[j],
                        'correlation': corr_value
                    })

        # ì‹œê°„ëŒ€ë³„ íŒ¨í„´ ë¶„ì„
        peak_hours = {}
        for log_type in self.df['log_type'].unique():
            type_data = self.df[self.df['log_type'] == log_type]
            hourly_counts = type_data['hour'].value_counts()
            if len(hourly_counts) > 0:
                peak_hours[log_type] = {
                    'peak_hour': hourly_counts.index[0],
                    'peak_count': hourly_counts.iloc[0],
                    'total_count': len(type_data)
                }

        summary = f"""
â° ì‹œê°„ì  ìƒê´€ê´€ê³„ ë¶„ì„
{'=' * 30}
â€¢ ë¶„ì„ ë¡œê·¸ íƒ€ì…: {len(time_log_type.columns)}ê°œ
â€¢ ê°•í•œ ìƒê´€ê´€ê³„: {len(strong_correlations)}ìŒ
â€¢ ìƒê´€ê´€ê³„ ì„ê³„ê°’: {self.thresholds['correlation_threshold']}
"""

        if strong_correlations:
            summary += "\nâ€¢ ì£¼ìš” ìƒê´€ê´€ê³„:\n"
            for corr in strong_correlations[:3]:
                summary += f"  - {corr['log_type_1']} â†” {corr['log_type_2']}: {corr['correlation']:.3f}\n"

        return {
            'correlation_matrix': correlation_matrix.to_dict(),
            'strong_correlations': strong_correlations,
            'peak_hours': peak_hours,
            'time_log_type_distribution': time_log_type.to_dict()
        }, summary

    def analyze_ip_clustering(self) -> Tuple[Dict, str]:
        """IP í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„"""
        if len(self.df) == 0:
            return {}, "IP í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # IPë³„ í–‰ìœ„ íŒ¨í„´ ë¶„ì„
        ip_patterns = {}

        for ip in self.df['source_ip'].dropna().unique():
            ip_data = self.df[self.df['source_ip'] == ip]

            if len(ip_data) < self.thresholds['clustering_min_events']:
                continue

            pattern = {
                'total_requests': len(ip_data),
                'log_types': ip_data['log_type'].value_counts().to_dict(),
                'severity_dist': ip_data['severity'].value_counts().to_dict(),
                'hourly_pattern': ip_data['hour'].value_counts().to_dict(),
                'active_hours': len(ip_data['hour'].unique()),
                'time_span_hours': (ip_data['timestamp'].max() - ip_data['timestamp'].min()).total_seconds() / 3600
            }

            ip_patterns[ip] = pattern

        # ìœ ì‚¬í•œ íŒ¨í„´ì„ ê°€ì§„ IP ê·¸ë£¹ ì°¾ê¸°
        ip_clusters = defaultdict(list)

        for ip1, pattern1 in ip_patterns.items():
            cluster_key = (
                len(pattern1['log_types']),  # ë¡œê·¸ íƒ€ì… ë‹¤ì–‘ì„±
                pattern1['active_hours'] // 4,  # í™œë™ ì‹œê°„ëŒ€ (4ì‹œê°„ ë‹¨ìœ„)
                'high' in pattern1['severity_dist']  # ê³ ìœ„í—˜ ì´ë²¤íŠ¸ ì¡´ì¬
            )
            ip_clusters[cluster_key].append((ip1, pattern1))

        # ì˜ë¯¸ìˆëŠ” í´ëŸ¬ìŠ¤í„°ë§Œ í•„í„°ë§ (2ê°œ ì´ìƒ IP)
        significant_clusters = {k: v for k, v in ip_clusters.items() if len(v) >= 2}

        summary = f"""
ğŸŒ IP í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
{'=' * 30}
â€¢ ë¶„ì„ IP ìˆ˜: {len(ip_patterns)}ê°œ
â€¢ ë°œê²¬ëœ í´ëŸ¬ìŠ¤í„°: {len(significant_clusters)}ê°œ
â€¢ ìµœì†Œ ì´ë²¤íŠ¸ ìˆ˜: {self.thresholds['clustering_min_events']}ê°œ
"""

        if significant_clusters:
            largest_cluster = max(significant_clusters.values(), key=len)
            summary += f"\nâ€¢ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° í¬ê¸°: {len(largest_cluster)}ê°œ IP"

        return {
            'ip_patterns': ip_patterns,
            'clusters': dict(significant_clusters),
            'cluster_stats': {k: len(v) for k, v in significant_clusters.items()}
        }, summary

    def analyze_attack_sequences(self) -> Tuple[Dict, str]:
        """ê³µê²© ì‹œí€€ìŠ¤ ë¶„ì„"""
        if len(self.df) == 0:
            return {}, "ê³µê²© ì‹œí€€ìŠ¤ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        # ê³µê²© ê´€ë ¨ ë¡œê·¸ í•„í„°ë§
        attack_keywords = ['failed', 'error', 'unauthorized', 'blocked', 'malware', 'suspicious']
        attack_logs = self.df[
            self.df['message'].str.lower().str.contains('|'.join(attack_keywords), na=False) |
            self.df['severity'].isin(['high', 'critical'])
            ].copy()

        if len(attack_logs) == 0:
            return {}, "ê³µê²© ê´€ë ¨ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."

        # IPë³„ ê³µê²© ì‹œí€€ìŠ¤ ë¶„ì„
        attack_sequences = []
        gap_threshold = timedelta(minutes=self.thresholds['sequence_gap_minutes'])

        for ip in attack_logs['source_ip'].dropna().unique():
            ip_attacks = attack_logs[attack_logs['source_ip'] == ip].sort_values('timestamp')

            if len(ip_attacks) < 2:
                continue

            # ì‹œí€€ìŠ¤ íƒì§€
            current_sequence = [ip_attacks.iloc[0]]

            for i in range(1, len(ip_attacks)):
                time_diff = ip_attacks.iloc[i]['timestamp'] - current_sequence[-1]['timestamp']

                if time_diff <= gap_threshold:
                    current_sequence.append(ip_attacks.iloc[i])
                else:
                    if len(current_sequence) >= 2:
                        sequence_info = {
                            'ip': ip,
                            'sequence_length': len(current_sequence),
                            'start_time': current_sequence[0]['timestamp'],
                            'end_time': current_sequence[-1]['timestamp'],
                            'duration': current_sequence[-1]['timestamp'] - current_sequence[0]['timestamp'],
                            'log_types': [event['log_type'] for event in current_sequence],
                            'severity_levels': [event['severity'] for event in current_sequence]
                        }
                        attack_sequences.append(sequence_info)
                    current_sequence = [ip_attacks.iloc[i]]

            # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì²˜ë¦¬
            if len(current_sequence) >= 2:
                sequence_info = {
                    'ip': ip,
                    'sequence_length': len(current_sequence),
                    'start_time': current_sequence[0]['timestamp'],
                    'end_time': current_sequence[-1]['timestamp'],
                    'duration': current_sequence[-1]['timestamp'] - current_sequence[0]['timestamp'],
                    'log_types': [event['log_type'] for event in current_sequence],
                    'severity_levels': [event['severity'] for event in current_sequence]
                }
                attack_sequences.append(sequence_info)

        # ìƒìœ„ ì‹œí€€ìŠ¤
        top_sequences = sorted(attack_sequences, key=lambda x: x['sequence_length'], reverse=True)[:5]

        summary = f"""
ğŸ” ê³µê²© ì‹œí€€ìŠ¤ ë¶„ì„
{'=' * 30}
â€¢ íƒì§€ëœ ì‹œí€€ìŠ¤: {len(attack_sequences)}ê°œ
â€¢ ê³µê²© ê´€ë ¨ ë¡œê·¸: {len(attack_logs)}ê°œ
"""

        if top_sequences:
            summary += "\nâ€¢ ì£¼ìš” ê³µê²© ì‹œí€€ìŠ¤:\n"
            for i, seq in enumerate(top_sequences[:3], 1):
                duration_min = seq['duration'].total_seconds() / 60
                summary += f"  {i}. {seq['ip']}: {seq['sequence_length']}ê°œ ì´ë²¤íŠ¸ ({duration_min:.1f}ë¶„)\n"

        return {'sequences': top_sequences, 'all_sequences': attack_sequences}, summary

    def plot_correlation_matrix(self, save_path: Optional[str] = None):
        """ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ì‹œê°í™”"""
        if len(self.df) == 0:
            print("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        fig, axes = plt.subplots(2, 2, figsize=(15, 12))

        # 1. ë¡œê·¸ íƒ€ì…ê°„ ì‹œê°„ì  ìƒê´€ê´€ê³„
        time_log_type = self.df.groupby(['hour', 'log_type']).size().unstack(fill_value=0)

        if len(time_log_type.columns) > 1:
            correlation_matrix = time_log_type.corr()
            sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu', center=0,
                        ax=axes[0, 0], fmt='.2f')
            axes[0, 0].set_title('ë¡œê·¸ íƒ€ì…ê°„ ì‹œê°„ì  ìƒê´€ê´€ê³„', fontweight='bold')
        else:
            axes[0, 0].text(0.5, 0.5, 'ìƒê´€ê´€ê³„ ë¶„ì„ ë¶ˆê°€\n(ë¡œê·¸ íƒ€ì… ë¶€ì¡±)',
                            transform=axes[0, 0].transAxes, ha='center', va='center')
            axes[0, 0].set_title('ë¡œê·¸ íƒ€ì…ê°„ ì‹œê°„ì  ìƒê´€ê´€ê³„', fontweight='bold')

        # 2. ì‹œê°„ëŒ€ë³„ ë¡œê·¸ íƒ€ì… ë¶„í¬
        log_type_counts = self.df['log_type'].value_counts()
        for log_type in log_type_counts.head(5).index:
            type_data = self.df[self.df['log_type'] == log_type]
            hourly = type_data['hour'].value_counts().sort_index()
            axes[0, 1].plot(hourly.index, hourly.values, marker='o', label=log_type, alpha=0.7)

        axes[0, 1].set_title('ì‹œê°„ëŒ€ë³„ ì£¼ìš” ë¡œê·¸ íƒ€ì… ë¶„í¬', fontweight='bold')
        axes[0, 1].set_xlabel('ì‹œê°„')
        axes[0, 1].set_ylabel('ë¡œê·¸ ìˆ˜')
        axes[0, 1].legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # 3. IPë³„ í™œë™ íŒ¨í„´ í´ëŸ¬ìŠ¤í„°ë§
        ip_analysis, _ = self.analyze_ip_clustering()
        ip_patterns = ip_analysis.get('ip_patterns', {})

        if ip_patterns:
            # IPë³„ í™œë™ ì‹œê°„ëŒ€ì™€ ìš”ì²­ ìˆ˜ ì‹œê°í™”
            ips = list(ip_patterns.keys())[:10]  # ìƒìœ„ 10ê°œ IPë§Œ
            request_counts = [ip_patterns[ip]['total_requests'] for ip in ips]
            active_hours = [ip_patterns[ip]['active_hours'] for ip in ips]

            scatter = axes[1, 0].scatter(active_hours, request_counts, alpha=0.6, s=60)
            axes[1, 0].set_xlabel('í™œë™ ì‹œê°„ëŒ€ ìˆ˜')
            axes[1, 0].set_ylabel('ì´ ìš”ì²­ ìˆ˜')
            axes[1, 0].set_title('IPë³„ í™œë™ íŒ¨í„´', fontweight='bold')

            # ì£¼ìš” IP ë¼ë²¨ë§
            for i, ip in enumerate(ips[:5]):
                axes[1, 0].annotate(ip, (active_hours[i], request_counts[i]),
                                    xytext=(5, 5), textcoords='offset points', fontsize=8)
        else:
            axes[1, 0].text(0.5, 0.5, 'IP í´ëŸ¬ìŠ¤í„°ë§ ë°ì´í„° ë¶€ì¡±',
                            transform=axes[1, 0].transAxes, ha='center', va='center')
            axes[1, 0].set_title('IPë³„ í™œë™ íŒ¨í„´', fontweight='bold')

        # 4. ê³µê²© ì‹œí€€ìŠ¤ ì‹œê°„ ë¶„í¬
        attack_analysis, _ = self.analyze_attack_sequences()
        sequences = attack_analysis.get('all_sequences', [])

        if sequences:
            durations = [(seq['duration'].total_seconds() / 60) for seq in sequences]
            lengths = [seq['sequence_length'] for seq in sequences]

            axes[1, 1].scatter(durations, lengths, alpha=0.6, color='red')
            axes[1, 1].set_xlabel('ì§€ì† ì‹œê°„ (ë¶„)')
            axes[1, 1].set_ylabel('ì‹œí€€ìŠ¤ ê¸¸ì´')
            axes[1, 1].set_title('ê³µê²© ì‹œí€€ìŠ¤ ë¶„ì„', fontweight='bold')
        else:
            axes[1, 1].text(0.5, 0.5, 'íƒì§€ëœ ê³µê²© ì‹œí€€ìŠ¤ ì—†ìŒ',
                            transform=axes[1, 1].transAxes, ha='center', va='center')
            axes[1, 1].set_title('ê³µê²© ì‹œí€€ìŠ¤ ë¶„ì„', fontweight='bold')

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.show()

    def generate_correlation_report(self) -> str:
        """ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        temporal_analysis, temporal_summary = self.analyze_temporal_correlation()
        ip_analysis, ip_summary = self.analyze_ip_clustering()
        attack_analysis, attack_summary = self.analyze_attack_sequences()

        report = f"""
{'=' * 60}
ğŸ”— ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸ (Django ORM)
{'=' * 60}

{self.get_correlation_overview_text()}

{'=' * 60}
{temporal_summary}
{'=' * 60}
"""

        # ê°•í•œ ìƒê´€ê´€ê³„ ìƒì„¸ ì •ë³´
        strong_correlations = temporal_analysis.get('strong_correlations', [])
        if strong_correlations:
            report += "ì£¼ìš” ìƒê´€ê´€ê³„:\n"
            for corr in strong_correlations:
                report += f"â€¢ {corr['log_type_1']} â†” {corr['log_type_2']}: {corr['correlation']:.3f}\n"

        # ì‹œê°„ëŒ€ë³„ í”¼í¬ ì •ë³´
        peak_hours = temporal_analysis.get('peak_hours', {})
        if peak_hours:
            report += "\në¡œê·¸ íƒ€ì…ë³„ í”¼í¬ ì‹œê°„:\n"
            for log_type, info in list(peak_hours.items())[:5]:
                report += f"â€¢ {log_type}: {info['peak_hour']}ì‹œ ({info['peak_count']}ê°œ)\n"

        report += f"""

{'=' * 60}
{ip_summary}
{'=' * 60}
"""

        clusters = ip_analysis.get('clusters', {})
        if clusters:
            report += "IP í´ëŸ¬ìŠ¤í„° ì •ë³´:\n"
            for i, (cluster_key, ips) in enumerate(clusters.items(), 1):
                if i <= 3:  # ìƒìœ„ 3ê°œ í´ëŸ¬ìŠ¤í„°ë§Œ
                    report += f"â€¢ í´ëŸ¬ìŠ¤í„° {i}: {len(ips)}ê°œ IP\n"
                    report += f"  - íŠ¹ì„±: {cluster_key}\n"

        report += f"""

{'=' * 60}
{attack_summary}
{'=' * 60}
"""

        sequences = attack_analysis.get('sequences', [])
        for i, seq in enumerate(sequences[:5], 1):
            duration_min = seq['duration'].total_seconds() / 60
            report += f"\n{i}. {seq['ip']}\n"
            report += f"   â€¢ ì‹œí€€ìŠ¤ ê¸¸ì´: {seq['sequence_length']}ê°œ\n"
            report += f"   â€¢ ì§€ì† ì‹œê°„: {duration_min:.1f}ë¶„\n"
            report += f"   â€¢ ë¡œê·¸ íƒ€ì…: {', '.join(set(seq['log_types']))}\n"

        report += f"""

{'=' * 60}
ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Django ORM ê¸°ë°˜ ìƒê´€ê´€ê³„ ë¶„ì„
"""
        return report